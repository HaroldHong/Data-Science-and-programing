source('~/GitHub/Data-Science-and-programing/week5/test1014.R', echo=TRUE)
source('~/GitHub/Data-Science-and-programing/week5/test1014.R', echo=TRUE)
source('~/GitHub/Data-Science-and-programing/week5/test1014.R', echo=TRUE)
source('~/GitHub/Data-Science-and-programing/week5/test1014.R', echo=TRUE)
source('~/GitHub/Data-Science-and-programing/week5/test1014.R', echo=TRUE)
source('~/GitHub/Data-Science-and-programing/week5/test1014.R', echo=TRUE)
source('~/GitHub/Data-Science-and-programing/week5/test1014.R', echo=TRUE)
source('~/GitHub/Data-Science-and-programing/week5/test1014.R', echo=TRUE)
source('~/GitHub/Data-Science-and-programing/week5/test1014.R', echo=TRUE)
source('~/GitHub/Data-Science-and-programing/week5/test1014.R', echo=TRUE)
source('~/GitHub/Data-Science-and-programing/week5/test1014.R', echo=TRUE)
source('~/GitHub/Data-Science-and-programing/week5/test1014.R', echo=TRUE)
source('~/GitHub/Data-Science-and-programing/week5/test1014.R', echo=TRUE)
source('~/GitHub/Data-Science-and-programing/week5/test1014.R', echo=TRUE)
source('~/GitHub/Data-Science-and-programing/week5/test1014.R', echo=TRUE)
mapply(collecturl1,10:31)
source('~/GitHub/Data-Science-and-programing/week5/test1014.R', echo=TRUE)
source('~/GitHub/Data-Science-and-programing/week5/test1014.R', echo=TRUE)
source('~/GitHub/Data-Science-and-programing/week5/test1014.R', echo=TRUE)
source('~/GitHub/Data-Science-and-programing/week5/textAnylise.R', echo=TRUE)
source('~/GitHub/Data-Science-and-programing/week5/textAnylise.R', echo=TRUE)
source('~/GitHub/Data-Science-and-programing/week5/textAnylise.R', echo=TRUE)
source('~/GitHub/Data-Science-and-programing/week5/textAnylise.R', echo=TRUE)
source('~/GitHub/Data-Science-and-programing/week5/textAnylise.R', echo=TRUE)
mapply(collecturl1,4)
mapply(collecturl1,4)
mapply(collecturl2,30)
source('~/GitHub/Data-Science-and-programing/week5/textAnylise.R', echo=TRUE)
source('~/GitHub/Data-Science-and-programing/week5/textAnylise.R', echo=TRUE)
source('~/GitHub/Data-Science-and-programing/week5/textAnylise.R', echo=TRUE)
mapply(traverseTEXT,5:30)
mapply(collecturl1,5)
mapply(traverseTEXT,5:30)
mapply(traverseTEXT,8:30)
mapply(traverseTEXT,10:30)
mapply(traverseTEXT,11:30)
mapply(collecturl2,11)
mapply(traverseTEXT,11:30)
mapply(collecturl2,12:13)
mapply(traverseTEXT,12:30)
mapply(collecturl2,13)
mapply(traverseTEXT,14:30)
mapply(collecturl2,21)
mapply(traverseTEXT,21:30)
mapply(collecturl2,24)
mapply(traverseTEXT,24:30)
mapply(collecturl2,27)
mapply(traverseTEXT,27:30)
mapply(collecturl2,27)
mapply(collecturl2,27)
mapply(collecturl2,27)
mapply(traverseTEXT,27:30)
source('~/.active-rstudio-document', echo=TRUE)
install.packages("proxy")
source('~/.active-rstudio-document', echo=TRUE)
install.packages("readtext")
source('~/.active-rstudio-document', echo=TRUE)
install.packages("tidytext")
source('~/.active-rstudio-document', echo=TRUE)
rawData = readtext("text091.txt")
docs = Corpus(VectorSource(rawData$text))
# data clean
toSpace <- content_transformer(function(x, pattern) {
return (gsub(pattern, " ", x))
})
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, toSpace, "[a-zA-Z]")
# words cut
keywords = read.csv("keywords.csv")
jieba_tokenizer = function(d){
unlist(segment(d[[1]], mixseg))
}
seg = lapply(docs, jieba_tokenizer)
# words cut
# keywords = read.csv("keywords.csv")
mixseg = worker()
jieba_tokenizer = function(d){
unlist(segment(d[[1]], mixseg))
}
seg = lapply(docs, jieba_tokenizer)
freqFrame = as.data.frame(table(unlist(seg)))
d.corpus <- Corpus(VectorSource(seg))
tdm <- TermDocumentMatrix(d.corpus)
print( tf <- as.matrix(tdm) )
DF <- tidy(tf)
# tf-idf computation
N = tdm$ncol
tf <- apply(tdm, 2, sum)
idfCal <- function(word_doc)
{
log2( N / nnzero(word_doc) )
}
idf <- apply(tdm, 1, idfCal)
doc.tfidf <- as.matrix(tdm)
for(x in 1:nrow(tdm))
{
for(y in 1:ncol(tdm))
{
doc.tfidf[x,y] <- (doc.tfidf[x,y] / tf[y]) * idf[x]
}
}
findZeroId = as.matrix(apply(doc.tfidf, 1, sum))
tfidfnn = doc.tfidf[-which(findZeroId == 0),]
write.csv(tfidfnn, "show.csv")
View(doc.tfidf)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', encoding = 'UTF-8', echo=TRUE)
source('~/.active-rstudio-document', encoding = 'UTF-8', echo=TRUE)
wordcloud2(d$word,size = 0.9,fontFamily = '微软雅黑')
source('~/.active-rstudio-document', encoding = 'UTF-8', echo=TRUE)
inspect(seg.unlist)
print(seg.unlist)
source('~/.active-rstudio-document', encoding = 'UTF-8', echo=TRUE)
source('~/.active-rstudio-document', encoding = 'UTF-8', echo=TRUE)
source('~/.active-rstudio-document', encoding = 'UTF-8', echo=TRUE)
#cloudword <- topsort(seg.unlist)
wordcloud2(freq_fram,size = 0.9,fontFamily = '微软雅黑')
source('~/.active-rstudio-document', encoding = 'UTF-8', echo=TRUE)
source('~/.active-rstudio-document', encoding = 'UTF-8', echo=TRUE)
source('~/.active-rstudio-document', encoding = 'UTF-8', echo=TRUE)
source('~/.active-rstudio-document', encoding = 'UTF-8', echo=TRUE)
library('xml2')
library('rvest')
library('dplyr')
collectTEXT <- function(url,i){
url_char <- as.character(url)
print(url_char)
web <- read_html(url_char,encoding = 'UTF-8')
text_node <- html_nodes(web,"#artibody p")
text_info <- html_text(text_node)
filepath <- paste("C:\\Users\\Harold\\Documents\\GitHub\\Data-Science-and-programing\\week5\\text09",i,".txt",sep = "")
cat(text_info, file = filepath,append = TRUE)
}
traverseTEXT <- function(i){
urlSetPath = "C:\\Users\\Harold\\Documents\\GitHub\\Data-Science-and-programing\\week5\\news_09"
urlSetPath <- paste(urlSetPath,i,".csv",sep = "")
urlframe <- read.csv(urlSetPath)
mapply(collectTEXT,urlframe$URL,i)
}
mapply(traverseTEXT,1:30)
library(NLP)
library(tm)
library(stats)
library(proxy)
library(dplyr)
library(readtext)
library(jiebaRD)
library(jiebaR)
library(slam)
library(Matrix)
library(tidytext)
topsort <- function(x,num = 0)
{
top <- count(x);
top <- top[order(top$freq,decreasing = T),]
if(num > 0){
return(top[1:num,])
}
else return(top)
}
docs <- Corpus(DirSource("C:\\Users\\Harold\\Documents\\GitHub\\Data-Science-and-programing\\week5\\text\\decline"),readerControl = list(language = "zho"))
writeLines(as.character(docs[[1]]))
docs <- tm_map(docs,toSpace, "黄金")
docs <- tm_map(docs,stripWhitespace)
docs <- tm_map(docs,removeNumbers)
docs <- tm_map(docs,stripWhitespace)
docs <- tm_map(docs,stripWhitespace)
mixseg = worker(stop_word = "C:\\Users\\Harold\\Documents\\GitHub\\Data-Science-and-programing\\week5\\mystopwords.txt")
jieba_tokenizer = function(d){
unlist(segment(d[[1]], jiebar = mixseg))
}
seg = lapply(docs, jieba_tokenizer)
freq_fram <- as.data.frame(table(unlist(seg)))
freq_fram <- freq_fram[order(freq_fram$Freq,decreasing = T),]
#剔除单个字符
freq_fram <- freq_fram[-which(nchar(as.character(freq_fram$Var1))==1),]
summary(seg)
d.corpus <- Corpus(VectorSource(seg))
s.dtm <- TermDocumentMatrix(d.corpus)
inspect(s.dtm)
tf <- apply(s.dtm,2,sum)
library(RColorBrewer)
library(wordcloud2)
m1 <- as.matrix(s.dtm)
v <- sort(rowSums(m1),decreasing = T)
d <- data.frame(word = names(v),freq = v)
seg.unlist <- unlist(seg)
wordcloud2(freq_fram,size = 0.9,fontFamily = '微软雅黑')
library(NLP)
library(tm)
library(stats)
library(proxy)
library(dplyr)
library(readtext)
library(jiebaRD)
library(jiebaR)
library(slam)
library(Matrix)
library(tidytext)
topsort <- function(x,num = 0)
{
top <- count(x);
top <- top[order(top$freq,decreasing = T),]
if(num > 0){
return(top[1:num,])
}
else return(top)
}
docs <- Corpus(DirSource("C:\\Users\\Harold\\Documents\\GitHub\\Data-Science-and-programing\\week5\\text\\decline"),readerControl = list(language = "zho"))
writeLines(as.character(docs[[1]]))
docs <- tm_map(docs,toSpace, "黄金")
docs <- tm_map(docs,stripWhitespace)
docs <- tm_map(docs,removeNumbers)
docs <- tm_map(docs,stripWhitespace)
docs <- tm_map(docs,stripWhitespace)
mixseg = worker(stop_word = "C:\\Users\\Harold\\Documents\\GitHub\\Data-Science-and-programing\\week5\\mystopwords.txt")
freq_fram <- freq_fram[order(freq_fram$Freq,decreasing = T),]
#剔除单个字符
freq_fram <- freq_fram[-which(nchar(as.character(freq_fram$Var1))==1),]
summary(seg)
d.corpus <- Corpus(VectorSource(seg))
d.corpus <- Corpus(VectorSource(seg))
s.dtm <- TermDocumentMatrix(d.corpus)
s.dtm <- TermDocumentMatrix(d.corpus)
inspect(s.dtm)
tf <- apply(s.dtm,2,sum)
library(RColorBrewer)
library(wordcloud2)
library(wordcloud2)
m1 <- as.matrix(s.dtm)
v <- sort(rowSums(m1),decreasing = T)
d <- data.frame(word = names(v),freq = v)
seg.unlist <- unlist(seg)
wordcloud2(freq_fram,size = 0.9,fontFamily = '微软雅黑')
